{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "from scipy.special import expit # The logistic sigmoid function \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "We will predict the incidence of diabetes based on various measurements (see [description](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)). Instead of directly using the raw data, we use a normalised version, where the label to be predicted (the incidence of diabetes) is in the first column. Download the data from [mldata.org](http://mldata.org/repository/data/download/csv/diabetes_scale/).\n",
    "\n",
    "Read in the data using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>num preg</th>\n",
       "      <th>plasma</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin fold</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.487437</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>-0.531170</td>\n",
       "      <td>-0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.145729</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>-0.414141</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.207153</td>\n",
       "      <td>-0.766866</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>0.839196</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.305514</td>\n",
       "      <td>-0.492741</td>\n",
       "      <td>-0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.105528</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>-0.535354</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-0.162444</td>\n",
       "      <td>-0.923997</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.376884</td>\n",
       "      <td>-0.344262</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.602837</td>\n",
       "      <td>0.284650</td>\n",
       "      <td>0.887276</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diabetes  num preg    plasma        bp  skin fold   insulin       bmi  \\\n",
       "0         0 -0.294118  0.487437  0.180328  -0.292929 -1.000000  0.001490   \n",
       "1         1 -0.882353 -0.145729  0.081967  -0.414141 -1.000000 -0.207153   \n",
       "2         0 -0.058824  0.839196  0.049180  -1.000000 -1.000000 -0.305514   \n",
       "3         1 -0.882353 -0.105528  0.081967  -0.535354 -0.777778 -0.162444   \n",
       "4         0 -1.000000  0.376884 -0.344262  -0.292929 -0.602837  0.284650   \n",
       "\n",
       "   pedigree       age  \n",
       "0 -0.531170 -0.033333  \n",
       "1 -0.766866 -0.666667  \n",
       "2 -0.492741 -0.633333  \n",
       "3 -0.923997 -1.000000  \n",
       "4  0.887276 -0.600000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['diabetes', 'num preg', 'plasma', 'bp', 'skin fold', 'insulin', 'bmi', 'pedigree', 'age']\n",
    "data = pd.read_csv('diabetes_scale.csv', header=None, names=names)\n",
    "data['diabetes'].replace(-1, 0, inplace=True) # The target variable need be 1 or 0, not 1 or -1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification via Logistic Regression\n",
    "\n",
    "Implement binary classification using logistic regression for a data set with two classes. Make sure you use appropriate [python style](https://www.python.org/dev/peps/pep-0008/) and [docstrings](https://www.python.org/dev/peps/pep-0257/).\n",
    "\n",
    "Use ```scipy.optimize.fmin_bfgs``` to optimise your cost function. ```fmin_bfgs``` requires the cost function to be optimised, and the gradient of this cost function. Implement these two functions as ```cost``` and ```grad``` by following the equations in the lectures.\n",
    "\n",
    "Implement the function ```train``` that takes a matrix of examples, and a vector of labels, and returns the maximum likelihood weight vector for logistic regresssion. Also implement a function ```test``` that takes this maximum likelihood weight vector and the a matrix of examples, and returns the predictions. See the section **Putting everything together** below for expected usage.\n",
    "\n",
    "We add an extra column of ones to represent the constant basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>num preg</th>\n",
       "      <th>plasma</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin fold</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>ones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.487437</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>-0.531170</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.145729</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>-0.414141</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.207153</td>\n",
       "      <td>-0.766866</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>0.839196</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.305514</td>\n",
       "      <td>-0.492741</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.105528</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>-0.535354</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-0.162444</td>\n",
       "      <td>-0.923997</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.376884</td>\n",
       "      <td>-0.344262</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.602837</td>\n",
       "      <td>0.284650</td>\n",
       "      <td>0.887276</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diabetes  num preg    plasma        bp  skin fold   insulin       bmi  \\\n",
       "0         0 -0.294118  0.487437  0.180328  -0.292929 -1.000000  0.001490   \n",
       "1         1 -0.882353 -0.145729  0.081967  -0.414141 -1.000000 -0.207153   \n",
       "2         0 -0.058824  0.839196  0.049180  -1.000000 -1.000000 -0.305514   \n",
       "3         1 -0.882353 -0.105528  0.081967  -0.535354 -0.777778 -0.162444   \n",
       "4         0 -1.000000  0.376884 -0.344262  -0.292929 -0.602837  0.284650   \n",
       "\n",
       "   pedigree       age  ones  \n",
       "0 -0.531170 -0.033333   1.0  \n",
       "1 -0.766866 -0.666667   1.0  \n",
       "2 -0.492741 -0.633333   1.0  \n",
       "3 -0.923997 -1.000000   1.0  \n",
       "4  0.887276 -0.600000   1.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ones'] = np.ones((data.shape[0], 1)) # Add a column of ones\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(w, X, y):\n",
    "    \"\"\"\n",
    "    Returns the cross-entropy error function.\n",
    "    \n",
    "    w -- parameters\n",
    "    X -- dataset of features where each row corresponds to a single sample\n",
    "    y -- dataset of labels where each row corresponds to a single sample\n",
    "    \"\"\"\n",
    "    outputs = expit(X.dot(w)) # Vector of outputs (or predictions)\n",
    "    return -( y.transpose().dot(np.log(outputs)) + (1-y).transpose().dot(np.log(1-outputs)) )\n",
    "\n",
    "def grad(w, X, y):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the cross-entropy error function.\n",
    "    \"\"\"\n",
    "    outputs = expit(X.dot(w))\n",
    "    return X.transpose().dot(outputs-y)\n",
    "    \n",
    "def train(X, y):\n",
    "    \"\"\"\n",
    "    Returns the vector of parameters which minimizes the cross-entropy error function via the BFGS algorithm.\n",
    "    \"\"\"\n",
    "    initial_values = np.zeros(X.shape[1]) # Error occurs if inital_values is set too high\n",
    "    return opt.fmin_bfgs(cost, initial_values, fprime=grad, args=(X,y))\n",
    "\n",
    "def predict(w, X):\n",
    "    \"\"\"\n",
    "    Returns a vector of predictions. \n",
    "    \"\"\"\n",
    "    return expit(X.dot(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance measure\n",
    "\n",
    "There are many ways to compute the [performance of a binary classifier](http://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers). The key concept is the idea of a confusion matrix or contingency table:\n",
    "\n",
    "|              |    | Label |    |\n",
    "|:-------------|:--:|:-----:|:--:|\n",
    "|              |    |  +1   | -1 |\n",
    "|**Prediction**| +1 |    TP | FP |\n",
    "|              | -1 |    FN | TN |\n",
    "\n",
    "where\n",
    "* TP - true positive\n",
    "* FP - false positive\n",
    "* FN - false negative\n",
    "* TN - true negative\n",
    "\n",
    "Implement three functions, the first one which returns the confusion matrix for comparing two lists (one set of predictions, and one set of labels). Then implement two functions that take the confusion matrix as input and returns the **accuracy** and **balanced accuracy** respectively. The [balanced accuracy](http://en.wikipedia.org/wiki/Accuracy_and_precision) is the average accuracy of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predictions, y): \n",
    "    \"\"\"\n",
    "    Returns the confusion matrix [[tp, fp], [fn, tn]].\n",
    "    \n",
    "    predictions -- dataset of predictions (or outputs) from a model\n",
    "    y -- dataset of labels where each row corresponds to a single sample\n",
    "    \"\"\"\n",
    "    tp, fp, fn, tn = 0, 0, 0, 0\n",
    "    predictions = predictions.round().values # Converts to numpy.ndarray\n",
    "    y = y.values\n",
    "    for prediction, label in zip(predictions, y):\n",
    "        if prediction == label: \n",
    "            if prediction == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else: \n",
    "            if prediction == 1:\n",
    "                fp += 1\n",
    "            else: \n",
    "                fn += 1\n",
    "    return np.array([[tp, fp], [fn, tn]])\n",
    "\n",
    "def accuracy(cm):\n",
    "    \"\"\"\n",
    "    Returns the accuracy, (tp + fp)/(tp + fp + fn + tn).  \n",
    "    \"\"\"\n",
    "    return cm.trace()/cm.sum()\n",
    "\n",
    "def balanced_accuracy(cm): \n",
    "    \"\"\"\n",
    "    Returns the balanced accuracy, (tp/p + tn/n)/2.\n",
    "    \"\"\"\n",
    "    return (cm[0,0]/(cm[0,0] + cm[0,1]) +  cm[1,1]/(cm[1,0] + cm[1,1]))/2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Consider the following code, which trains on all the examples, and predicts on the training set. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 361.722693\n",
      "         Iterations: 18\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.78255208333333337, 0.76912964680456408]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['diabetes']\n",
    "X = data[['num preg', 'plasma', 'bp', 'skin fold', 'insulin', 'bmi', 'pedigree', 'age', 'ones']]\n",
    "theta_best = train(X, y)\n",
    "pred = predict(theta_best, X)\n",
    "cmatrix = confusion_matrix(pred, y)\n",
    "[accuracy(cmatrix), balanced_accuracy(cmatrix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's discriminant\n",
    "\n",
    "In the lectures, you saw that the Fisher criterion\n",
    "$$\n",
    "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
    "$$\n",
    "is maximum for Fisher's linear discriminant.\n",
    "\n",
    "Define $S_B$ and $S_W$ as in the lectures and prove this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Effect of regularization parameter\n",
    "\n",
    "By splitting the data into two halves, train on one half and report performance on the second half. By repeating this experiment for different values of the regularization parameter $\\lambda$ we can get a feeling about the variability in the performance of the classifier due to regularization. Plot the values of accuracy and balanced accuracy for at least 3 different choices of $\\lambda$. Note that you may have to update your implementation of logistic regression to include the regularisation parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Solution\n",
    "\n",
    "def split_data(data):\n",
    "    \"\"\"Randomly split data into two equal groups\"\"\"\n",
    "    np.random.seed(1)\n",
    "    N = len(data)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:int(N/2)]\n",
    "    test_idx = idx[int(N/2):]\n",
    "\n",
    "    X_train = data.loc[train_idx].drop('diabetes', axis=1)\n",
    "    t_train = data.loc[train_idx]['diabetes']\n",
    "    X_test = data.loc[test_idx].drop('diabetes', axis=1)\n",
    "    t_test = data.loc[test_idx]['diabetes']\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
