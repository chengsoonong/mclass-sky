\documentclass[11pt,twoside]{report}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{cleveref}
\usepackage{color}
\usepackage{fontspec}
\usepackage{mathpazo}
\usepackage{mathtools}
\usepackage{minted}
\usepackage{wrapfig}
\usepackage[separate-uncertainty=true]{siunitx}

\usepackage[intoc, english]{nomencl}
\makenomenclature

\setmonofont[Mapping=tex-text]{Menlo}
\usemintedstyle{}

% This is for writing only, so it's easier to read.
\usepackage[paperheight=9.12in,paperwidth=6.389in,tmargin=.4in,bmargin=.6in,inner=.2in,outer=.4in,heightrounded]{geometry}

\renewcommand*{\thefootnote}{[\arabic{footnote}]}

\allowdisplaybreaks

%\setmainfont{Palatino Linotype}
% The Palatino package gives a pretty terrible look to the PDF when compiling on
% Windows, so we need to use fontspec instead. Palatino Linotype exists on
% Windows but not on Mac (where Palatino exists instead). So...
\suppressfontnotfounderror1
\def\myfont{Palatino}
\def\myfallback{Palatino Linotype}
\count255=\interactionmode
\batchmode
\font\foo="\myfont"\space at 10pt
\ifx\foo\nullfont
  \font\foo = "\myfallback"\space at 10pt
  \ifx\foo\nullfont
    \errorstopmodep
    \errmessage{no suitable font found}
  \else
    \let\myfont=\myfallback
  \fi
\fi
\interactionmode=\count255
\setmainfont[Ligatures=TeX]{\myfont}

% This will add the units
%----------------------------------------------
\newcommand{\nomunit}[1]{%
\renewcommand{\nomentryend}{\hspace*{\fill}#1}}
%----------------------------------------------


\DeclareSIUnit{\parsec}{pc}

% Define common symbols
\newcommand\bv{\mathbf{v}}
\newcommand\bw{\mathbf{w}}
\newcommand\bx{\mathbf{x}}
\newcommand\by{\mathbf{y}}
\newcommand\bphi{\bm{\phi}}
\newcommand\bvarphi{\bm{\varphi}}
\newcommand\bbE{\mathbb{E}}
\newcommand\bbN{\mathbb{N}}
\newcommand\bbR{\mathbb{R}}

\newcommand\cA{\mathcal{A}}
\newcommand\cB{\mathcal{B}}
\newcommand\cC{\mathcal{C}}
\newcommand\cD{\mathcal{D}}
\newcommand\cE{\mathcal{E}}
\newcommand\cF{\mathcal{F}}
\newcommand\cG{\mathcal{G}}
\newcommand\cN{\mathcal{N}}
\newcommand\cP{\mathcal{P}}
\newcommand\cS{\mathcal{S}}
\newcommand\cX{\mathcal{X}}

\newcommand\cov{\mathrm{cov}}
\newcommand\var{\mathrm{var}}

\newcommand\norm[1]{\left\|#1\right\|}
\newcommand\floor[1]{\left\lfloor#1\right\rfloor}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\spn}{span}

\newcommand\jakub[1]{{\color{red}(j: #1)}}

\title{Optimising Experiments for Photometric Redshift Prediction}
\author{Jakub Nabaglo, u5558578}
\date{October 2017}
\usepackage{layouts}

\begin{document}
\maketitle

\tableofcontents

\nomenclature{$\cX$}{Predictor input set}
\nomenclature{$\bbR$}{Set of real numbers}
\nomenclature{$f$}{Original function}
\nomenclature{$\hat f$}{Predictor}
\nomenclature{$n$}{Punctuation Symbol}
\nomenclature{$m$}{Punctuation Symbol}
\nomenclature{$d$}{Punctuation Symbol}
\nomenclature{$c$}{Speed of light in a vacuum\nomunit{\SI{299792458}{\meter\per\second}}}
\nomenclature{$z$}{Redshift}
\nomenclature{$z_\mathrm{spec}$}{Redshift}
\nomenclature{$z_\mathrm{pred}$}{Redshift}

\printnomenclature


\chapter{Introduction}

\prntlen{\textwidth}

A redshift is the change in the wavelength of a photon emitted by an object that is moving away from the observer. When applied to the visible spectrum, a redshift makes an object appear more red.

Redshifts are important in astronomy, as they are used for a variety of tasks. The most important of these tasks is finding distances to objects.

Currently, redshifts are found using specroscopy. A spectroscope is applied to a galaxy, and its spectral profile is measured. From these measurements, we can isolate the hydrogen lines, which are then compared against the baseline to obtain the redshift. This method is problematic as it requires a spectroscope to be pointed at each galaxy whose redshift we wish to measure. Since there are many galaxies in the universe, it is impossible to use this method for large extents of the sky.

Instead, we can utilise a technique called `photometric redshift' prediction. A photometric redshift is one computed from photometric measurements, as opposed to spectroscopic ones. Photometric measurements `bucket' the spectrum into a small number of distinct intervals, much like a photographic camera buckets the visible spectum into red, green, and blue. These buckets can then be used as input to a regressor, and the redshift can be predicted.

Photometric have their own disadvantages. Currently, they only cover a limited range of redshifts. Further, they tend to rely on limited datasets, whose spectra have been measured without much regard to the impact of the sample on the prediction.

It therefore is appropriate to apply optimal design techniques to the problem. Optimal design allows us to choose, from a pool of unlabelled galaxies (i.e., galaxies for which we know the photometric data, but not the exact redshift from spectroscopic data), the one that would bring the most value to the regression problem if it were labelled. In other words, towards which galaxy should we point the spectroscope next so as to give us the most new information?

This method of selection of new training samples has advantages of greater cost effectiveness. It will also permit us to increase the range of current photometric redshift models with the lowest effort.

Optimal design requires us to be able to assess the uncertainties in the prediction for each point in the input space. This provides a useful heuristic for choosing the next sample to label.

\section{Redshift}
  Redshift is a property of all signals we observe from astronomical objects. It is often not easy to measure, but finding it permits us to learn important facts about the object being observed.

  \jakub{Caused not just by doppler effect. Also expansion of the universe.}

  The electromagnetic waves that we observe from distant objects are subject to the Doppler effect. This is the phenomenon by which waves produced by an object that is moving away from the observer appear to have a longer wavelength. Consider an ambulance driving away from you: the tone of the siren appears lower than if the ambulance was stationary. In a similar way, a galaxy moving away from us appears redder, giving rise to the term `redshift'. The converse effect, affecting objects moving towards the observer, is termed `blueshift'.

  The redshift of an object is then directly correlated to its velocity perpendicular to the line of sight from the observer. For a redshift $z$, the line-of-sight velocity is\[
      v \approx cz \text{,}
  \] where $c$ is the speed of light constant.

  This permits us to approximate the distance from the observed object. Due to the expansion of space, distant objects are moving away from us at a velocity proportional to their distance. For a velocity $v$, that distance is \[
      D \approx \frac{v}{H_0} \text{,}
  \] where $H_0$ is the Hubble parameter, equalling approximately $\SI{70}{\kilo\meter\per\second\per\mega\parsec}$. For faraway objects then, the redshift is hence proportional to distance.

  Since the speed of light is finite, electromagnetic waves coming from distant objects represent the past. Hence, observing distant objects permits us to study the history of the universe. For example, the universe is denser at larger distances, as a remnant of the earlier stages of the big bang. Similarly, large distances contain more quasars and more brighter objects. The ability to compute distance from the redshift is important in studying the past of the universe as the distance is a proxy for the age of the object we observe.

  The mass of an object is correlated to its luminosity\footnote{the total amount of light emitted}. The luminosity can be approximated from the brightness using the object's distance. Hence, computing the distance of an object from the redshift permits us to approximate its size.

  However, for some objects, we can approximate the size more directly. For closer objects, we can measure their apparent radius as the angle they span on the sky. Using trigonometry, we can then compute the actual radius of such an object from its apparent radius and redshift-derived distance.

  For closer galaxies, we are able to observe the movements of stars inside the galaxy. When a star orbits the centre of a galaxy, the internal motion can cancel or amplify the star's line-of-sight velocity. Measuring the stars' velocities from redshift hence permits us to to study galaxy dynamics.

\chapter{Photometry}

  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{zpec_hist.pdf}
    \caption{Histogram of objects in SDSS and 2dFLenS by spectroscopic redshift.}
    \label{fig:spec_hist}
  \end{figure}

  \jakub{define photometry}

  \section{Colours}

  \jakub{write about the 5 magnitudes used}

  \jakub{plot the filters}

  \jakub{why these 5 filters?}

  \jakub{getting canonical colours from the five filters}

  \section{Datasets}
  \jakub{TODO: WRITE}

  \subsection{SDSS}
  The spectroscopic redshift distribution of objects in SDSS is presented in \cref{fig:spec_hist}.

  \jakub{TODO: WRITE}

  \subsection{2dfLenS}
  The spectroscopic redshift distribution of objects in 2dfLenS is presented in \cref{fig:spec_hist}.

\chapter{Regression on Photometric Redshifts}

\jakub{define X}

Let $f$ be an unobserved function mapping $\cX$ to $\bbR$, where $\cX \subseteq \bbR^d$ for some $d \in \bbN$. Regression is the task of reconstructing such $f$ from finitely many samples. As a complication, these samples may be noisy.

More formally, we are given a set of $n$ samples\[
    \cD = \{(\bx_1, y_1), \dots (\bx_n, y_n)\} \text{.}
\] For each $i = 1, \dots, n$, \[
    y_i = f(\bx_i) + \epsilon \text{ for some error } \epsilon \sim \cP \text{,}
\] where $\cP$ is some probability distribution of the error.

We wish to find a function $\hat f : \bbR^d \to \bbR$ that maximises the likelihood \[
    p\left(\by \;\middle|\; \hat f, X\right) \text{,}
\] where we define\[
    \by \coloneqq \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}
    \text{ and }
    X \coloneqq \begin{bmatrix} \bx_1^\intercal \\ \vdots \\ \bx_n^\intercal \end{bmatrix}
\]

The formalisation of this likelihood is dependent on our assumptions. For example, it will depend on whether we know $\cP$, we assume it, or it is given to us.

In our case we will always assume that \[
    \cP = \cN(0, \sigma^2)
\] for some variance $\sigma^2$.

\section{Linear Regression}

  \begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{linreg_plain.pdf}
    \caption{Error of linear regression with linear features. Lower is better; error of $0$ means perfect fit. Note that since this regression is purely linear, a bias term is not used.}
    \label{fig:linreg_plain}
  \end{wrapfigure}

  \begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{linreg_polynomial.pdf}
    \caption{Error of linear regression with polynomial features. Note that degree 1 polynomial features are equivalent to linear regression with a bias term.}
    \label{fig:linreg_polynomial}
  \end{wrapfigure}

Linear regression is the simplest regression model. \jakub{is it?}

Choose $m \in \bbN$. Choose $\phi_1, \dots, \phi_m$, each mapping $\cX$ to $\bbR$.

We make the assumption that $\hat f$ is a linear combination of $\phi_1, \dots, \phi_m$. Then \[
    \hat f(\bx) = \bphi(\bx)^\top \bw \text{,}
\] where we define \[
    \bphi(\bx) \coloneqq \begin{bmatrix} \phi_1(\bx) \\ \vdots \\ \phi_n(\bx) \end{bmatrix}\text{,}
\] and where $\bw \in \bbR^m$ is the weights vector we wish to find.

Define also\[
    \Phi \coloneqq \begin{bmatrix}
        \bphi(\bx_1)^\top \\
        \vdots \\
        \bphi(\bx_m)^\top
    \end{bmatrix} \text{.}
\]

We also make the assumption that $p(\by \mid \hat f, X)$ depends only on how well $\hat f$ approximates $f$ at $x_1, \dots, x_n$. More formally, defining \[
    \hat y_i \coloneqq f(\bx_i) \text{ for }i = 1, \dots, n\text{ and }
    \hat\by \coloneqq \begin{bmatrix} \hat y_1 \\ \vdots \\ \hat y_n \end{bmatrix} \text{,}
\] we have that
\begin{align}
    p(\by \mid \hat f, X) &= \prod_{i = 1}^{n} \cN\left(\hat y_i \;\middle|\; y_i, \sigma^2\right) \label{eq:linearlikelihood} \\
    &= \prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right) \text. \nonumber
\end{align} We wish to find $\bw$ that maximises this likelihood. This is equivalent to minimising the negative log likelihood,\begin{align*}
    -\log p(\by \mid \hat f, X) &= - \log \prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right) \\
    &= - \sum_{i = 1}^{n} \log\left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right)\right) \\
    &= - \sum_{i = 1}^{n} \left(\log\frac{1}{\sqrt{2\pi\sigma^2}} + \log\exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right)\right) \\
    &= - \sum_{i = 1}^{n} \left(-\frac{1}{2}\log\left(2\pi\sigma^2\right) -\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right) \\
    &=  \frac{n}{2}\log\left(2\pi\sigma^2\right) + \frac{1}{2\sigma^2}\sum_{i = 1}^{n}\left(\hat y_i - y_i\right)^2 \text{.}
\end{align*} Since $\frac{n}{2}\log(2\pi\sigma^2)$ is a constant, minimising $-\log p(\by \mid \hat f, X)$ is equivalent to minimising $\frac{1}{2\sigma^2}\sum_{i = 1}^{n}(\hat y_i - y_i)^2$. Further, since $1/\sigma^2$ is constant, this is equivalent to minimising $\frac12\sum_{i = 1}^{n}(\hat y_i - y_i)^2$.

We call this our loss function and write\[
    E(\bw) = \frac12\sum_{i = 1}^{n}(\hat y_i(\bw) - y_i)^2 \text{.}
\] Note that we can compute it without knowing $\sigma^2$. For reasons that should be clear, this is called the \textit{sum of squares} error.

The optimal weights vector is then given by \[
    \argmax_{\bw} p(\by \mid \hat f(\bw), X) = \argmin_{\bw} E(\bw) \text{.}
\]

We can solve this analytically. We first note that \begin{align*}
    E(\bw) &= \frac12\sum_{i = 1}^{n}(\bphi(\bx_i)^\top \bw - y_i)^2 \\
    &= \frac12\sum_{i = 1}^{n}(\Phi \bw - \by)_i^2 \\
    &= \frac12(\Phi \bw - \by)^\top(\Phi \bw - \by) \text{.}
\end{align*} Setting the derivative to zero to find the minimum, \[
    \frac{dE(\bw)}{d\bw} = \Phi^\top (\Phi\bw - \by) = \mathbf{0} \text{.}
\] Solving, the minimum is \[
    \bw  = \left(\Phi^\top\Phi\right)^{-1}\Phi^\top\by
\]

\subsubsection{Feature differences}

We show that create new features that are pairwise differences between existing features does not affect a linear model since the same predictor can be found using the original features.

Consider a dataset with $n$ features. Let $\mathbf{x}$ be the $n$-dimensional feature vector.

Let $\mathbf{D}$ be an $n \times n$ matrix such that \[
    \mathbf{D}_{ij} \coloneqq \mathbf{x}_i - \mathbf{x}_j \text{.}
\]

Let $\bm{\alpha}$ be an $n$-dimensional weights vector for $\mathbf{x}$ and let $\mathcal{B}$ be an $n \times n$ weights matrix for $\mathbf{D}$. Let our predictor be\[
    y = \bm{\alpha} \cdot \mathbf{x} + \langle \mathcal{B}, \mathbf{D} \rangle_F \text{,}\label{eq:pred1}\tag{$*$}
\] where $\langle \mathcal{B}, \mathbf{D} \rangle_F$ is a Frobenius inner product.

Let $\hat{\bm{\alpha}}$ be an $n$-dimensional vector such that \[
    \hat{\bm{\alpha}}_i \coloneqq \bm{\alpha}_i + \sum_{j=1}^n \mathcal{B}_{ij} - \sum_{j=1}^n \mathcal{B}_{ji} \text{.}
\] We show that a predictor defined as \[
    y = \hat{\bm{\alpha}} \cdot \mathbf{x}
\] is equivalent to \eqref{eq:pred1}.

We have that\begin{align*}
    \hat{\bm{\alpha}} \cdot \mathbf{x} &= \sum_{i=1}^n \mathbf{x}_i\left( \bm{\alpha}_i + \sum_{j=1}^n \mathcal{B}_{ij} - \sum_{j=1}^n \mathcal{B}_{ji} \right) \\
    &= \sum_{i=1}^n \mathbf{x}_i\bm{\alpha}_i +  \sum_{i=1}^n \sum_{j=1}^n \mathbf{x}_i \mathcal{B}_{ij} - \sum_{i=1}^n \sum_{j=1}^n \mathbf{x}_i \mathcal{B}_{ji} \\
    &= \sum_{i=1}^n \mathbf{x}_i\bm{\alpha}_i +  \sum_{i=1}^n \sum_{j=1}^n \mathbf{x}_i \mathcal{B}_{ij} - \sum_{i=1}^n \sum_{j=1}^n \mathbf{x}_j \mathcal{B}_{ij} \\
    &= \bm{\alpha}\cdot\mathbf{x} +  \sum_{i=1}^n \sum_{j=1}^n \mathcal{B}_{ij} \left(\mathbf{x}_i - \mathbf{x}_j \right) \\
    &= \bm{\alpha}\cdot\mathbf{x} + \langle \mathcal{B}, \mathbf{D} \rangle_F
\end{align*} as desired.

Note that this does not apply to non-linear models where a nonlinear transformation is applied to $\mathbf{D}$. It also does not necessarily hold when regularisation is applied to the weights.

\subsection{Regularisation}

  \begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{linreg_kernelised_regularised.pdf}
    \caption{Error of regularised kernelised linear regression. The regularisation constant $\lambda$ is $0.001$.}
    \label{fig:linreg_kernelised_regularised}
  \end{wrapfigure}

Often we wish to limit the complexity of $\hat f$ to prevent overfitting. We can make the assumption that every element of the weights vector $\bw$ is likely to be close to zero: \[
    p(w_i) = \cN(w_i \mid 0, \mu)
\] for some variance $\mu > 0$.

We can then change the likelihood in \cref{eq:linearlikelihood} to account for this: \begin{align*}
    p(\by \mid \hat f, X) &= \prod_{i = 1}^{n} \cN\left(\hat y_i \;\middle|\; y_i, \sigma^2\right) \label{eq:linearlikelihood} \prod_{i = 1}^{m}\cN(w_i \mid 0, \mu) \\
    &= \prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right) \prod_{i = 1}^{m}\frac{1}{\sqrt{2\pi\mu}} \exp\left(-\frac{w_i^2}{2\mu}\right) \text{.}
\end{align*}

Computing the negative log likelihood,\[
    -\log p(\by \mid \hat f, X) = \frac{n}{2}\log\left(2\pi\sigma^2\right) + \frac{1}{2\sigma^2}\sum_{i=1}^{n}\left(\hat y_i - y_i\right)^2 + \frac{m}{2}\log\left(2\pi\mu\right) + \frac{1}{2\mu}\sum_{i=1}^{m}w_i^2 \text{.}
\]

Eliminating the constant terms, we find that \begin{align*}
    \argmin_{\bw} - \log p\left(\by \;\middle|\; \hat f, X\right) &= \argmin_{\bw} \left(\frac{1}{2\sigma^2}\sum_{i=1}^{n}\left(\hat y_i - y_i\right)^2 + \frac{1}{2\mu}\sum_{i=1}^{m}w_i^2\right) \\
    &= \argmin_{\bw} \frac{1}{\sigma^2}\left(\frac12\sum_{i=1}^{n}\left(\hat y_i - y_i\right)^2 + \frac{\lambda}{2}\sum_{i=1}^{m}w_i^2\right) \text{,}
\end{align*} where we defined $\lambda \coloneqq \sigma^2 / \mu > 0$. Eliminating the positive multiplicative constant,\begin{align*}
    \argmin_{\bw} - \log p\left(\by \;\middle|\; \hat f, X\right) &= \argmin_{\bw} \left(\frac12\sum
    _{i=1}^{n}\left(\hat y_i - y_i\right)^2 + \frac{\lambda}{2}\sum_{i=1}^{m}w_i^2\right) \\
    &= \argmin_{\bw} \left(\frac12\left(\hat \by - \by\right)^\top\left(\hat \by - \by\right) + \frac{\lambda}{2}\bw^\top\bw\right) \\
    &= \argmin_{\bw} \left(\frac12\left(\Phi \bw - \by\right)^\top\left(\Phi \bw - \by\right) + \frac{\lambda}{2}\bw^\top\bw\right) \text.
\end{align*}

Our loss now depends on $\bw$ and $\lambda$ and is given by \[
    E(\bw, \lambda) = \frac12\left(\Phi \bw - \by\right)^\top\left(\Phi \bw - \by\right) + \frac{\lambda}{2}\bw^\top\bw \text{,}
\] where $\lambda$ is our \textit{regularisation constant}: a higher value implies a simpler model, reducing the likelihood of overfitting, but increasing the chance of underfitting.

We can find $\bw$ that minimises the loss. Taking the derivative and setting it to $\mathbf{0}$,\[
    \frac{dE(\bw, \lambda)}{d\bw} = \Phi^\top(\Phi\bw - \by) + \lambda \bw = \mathbf{0} \text{.}
\] Solving,\[
    \bw = \left(\Phi^\top\Phi + \lambda I\right)^{-1}\Phi^\top\by \text.
\]

\section{Kernels}

A kernel is a function $k : \bbR^d \times \bbR^d \to \bbR$ such that \[
    k(\bx, \bx') = \langle \bvarphi\left(\bx\right), \bvarphi\left(\bx'\right)\rangle \text{,}
\] where $\bvarphi : \bbR^d \to H$ maps our feature space $\bbR^d$ to a real Hilbert space $H$. Intuitively then, we can think of a kernel as an inner product in a different space.

Like other inner products, a kernel can be thought of as a similarity measure. For example, $k(\bx, \bx') = 0$, then $\bx$ and $\bx'$ are orthogonal in $H$, and thus not similar at all by our definition.

Kernels permit us to generalise our earlier basis transformations $\bphi : \bbR^d \to \bbR^m$ to $\bvarphi : \bbR^d \to H$. Due to the possibly infinite-dimensional nature of $H$, we cannot use $\bvarphi$ directly. Instead, we can take advantage of it by computing $k$.

\subsection{Examples of Kernels}

\jakub{TODO: make very helpful plots}

\subsubsection{Gaussian Kernel}

  \begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{gaussian_kernel_plot.pdf}
    \caption{$K(0, x)$, where $K$ is the Gaussian kernel.}
    \label{fig:gaussian_kernel}
  \end{wrapfigure}

The Gaussian Kernel is defined by \[
    k(\bx, \bx') \coloneqq \exp\left( - \frac{\norm{\bx - \bx'}^2}{2\sigma^2} \right) \text{,}
\] where the hyperparamerer $\sigma$ is known as the \textit{length scale}. This is visualised in \cref{fig:gaussian_kernel}.

An alternative definition involves length scales $\sigma_1, \dots, \sigma_d$, permitting us to set the length independently for each axis. The definition is \[
    k(\bx, \bx') \coloneqq \exp\left( - \frac{1}{2}\left(\bx - \bx'\right)^T\Sigma^{-1}\left(\bx - \bx'\right) \right) \text{,}
\] where \[
    \Sigma = \begin{bmatrix}
        \sigma_1^2 & & \makebox(0,0){\text{\huge0}} \\
        & \ddots & \\
        \mbox{\huge 0} & & \sigma_d^2
    \end{bmatrix} \text{.}
\]

\subsubsection{Rational-Quadratic Kernel}

  \begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{rational_quadratic_kernel_plot.pdf}
    \caption{$K(0, x)$, where $K$ is the Rational-Quadratic kernel.}
    \label{fig:rational_quadratic_kernel}
  \end{wrapfigure}

The Rational-Quadratic Kernel is defined by \[
    k(\bx, \bx') \coloneqq \left(1 + \frac{1}{2\gamma}\norm{\bx - \bx'}^2\right)^{-\beta} \text{,}
\] where $\gamma > 0$ is the \textit{length scale} and $\beta > 0$ is the \textit{shape parameter}. It can be thought of as an infinite sum of gaussian kernels.

The Rational-Quadratick Kernel is visualised in \cref{fig:rational_quadratic_kernel}.

Setting $\gamma = \beta \gamma_0$ and letting limiting $\beta \to \infty$ yields a Gaussian kernel with length scale $\gamma_0$.

\subsubsection{Mat\'ern Kernel}

  \begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{matern_kernel_plot.pdf}
    \caption{$K(0, x)$, where $K$ is the Mat\'ern kernel.}
    \label{fig:matern_kernel}
  \end{wrapfigure}

The Mat\'ern Kernel is defined by \[
    k(\bx, \bx') \coloneqq \sigma^2\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}}{\ell}\norm{\bx - \bx'}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}}{\ell}\norm{\bx - \bx'}\right)
\]$K_\nu$ is the modified Bessel function of the second kind, $\Gamma$ is the Gamma function, $\nu$ is positive parameter, and $\ell$ is the length scale. It is differentiable $\floor{\nu-1}$ times.

See \cref{fig:matern_kernel} for a visualisation.

\jakub{Something about the physical meaning would be nice}

\subsubsection{Exponential Kernel}

  \begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{exponential_kernel_plot.pdf}
    \caption{$K(0, x)$, where $K$ is the Exponential kernel.}
    \label{fig:exponential_kernel}
  \end{wrapfigure}

\jakub{TOOD: write}

The exponential kernel is visualised in \cref{fig:exponential_kernel}.

\subsection{Kernel Approximations}

For large dataset, computing $K$ becomes expensive as has a space complexity of $O(n^2)$.

\subsubsection{RBF Sampling}

RBF Sampling approximates the Gaussian Kernel by sampling from its Fourier transform. \jakub{I literally don't know if this is correct}

\jakub{prove}

\subsubsection{Empirical Kernel Map}

\jakub{TODO: WRITE}


\subsection{Kernelised Linear Regression}

  \begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{linreg_kernelised.pdf}
    \caption{Error of kernelised linear regression. The $\sigma$ is the median distance between training $\bx$ values.}
    \label{fig:linreg_kernelised}
  \end{wrapfigure}

Consider the problem of regression. We are given a function $f : \bbR^d \to \bbR$ for some $d \in \bbN$. We are also given $n$ points $\bx_1, \dots, \bx_n$ for which we know the values. In other words, we know $y_1, \dots, y_n$ where \[
    y_i \coloneqq f(\bx_i)\text{ for }i = 1, \dots, n\text{.}
\]

Let $\phi : \bbR^d \to \bbR^r$ be the feature map. If we are performing linear regression, then we want to find $\bw \in \bbR^r$ such that \[
    f(\bx) \approx \phi(\bx)^T \bw \text{.}
\]

Define \[
    \Phi \coloneqq \begin{bmatrix}
        \phi(\bx_1)^T \\
        \vdots \\
        \phi(\bx_n)^T \\
    \end{bmatrix} \text{ and }\by \coloneqq \begin{bmatrix}
        y_1 \\
        \vdots \\
        y_n
    \end{bmatrix} \text{.}
\]

We wish to use the least-squares error with $\ell_2$ regularisation. We can define the error function to be $E : \bbR^r \to [0, \infty)$ such that \[
    E(\bw) \coloneqq \frac{1}{2}\norm{\by - \Phi\bw}_2^2 + \frac{\lambda}{2} \norm{\bw}_2^2 \text{,}
\] where $\lambda > 0$ is the regularisation constant.

We know that $\bw \in \spn\{\phi(\bx_1), \dots, \phi(\bx_n)\}$. Hence, there exists $\bv \in \bbR^n$ such that \[
    \bw = \Phi^T\bv \text{.}
\]

We can rewrite the error function as $E : \bbR^n \to [0, \infty)$ such that \[
    E(\bv) = \frac{1}{2}\norm{\by - \Phi\Phi^T\bv}_2^2 + \frac{\lambda}{2} \norm{\Phi^T\bv}_2^2 \text{.}
\] Letting $K \coloneqq \Phi\Phi^T$ be the kernel matrix,\[
    E(\bv) = \frac{1}{2}\norm{\by - K\bv}_2^2 + \frac{\lambda}{2} \bv^T K \bv \text{.}
\] Then $k(\bx, \bx') \coloneqq \phi(\bx) \cdot \phi(\bx')$ is our kernel function.

Differentiating, we find \begin{align*}
    E'(\bv) &= -K^T(\by - K\bv) + \frac{\lambda}{2} (K + K^T)\bv \\
    &= K(K\bv - \by) + \lambda K\bv \text{,}
\end{align*} since $K$ is symmetric. The minimum exists at $E'(\bv) = 0$. Simplifying, we find,\[
    K(K + \lambda I)\bv = K\by \text{.}
\] There may be multiple solutions. One solution is at \[
    \bv = (K + \lambda I)^{-1}\by \text{.}
\]

For a given input $\bx$, the prediction is then \begin{align*}
    f(\bx) &\approx \phi(\bx)^T \bw \\
    &= \phi(\bx)^T \Phi \bv \\
    &= \phi(\bx)^T \Phi (K + \lambda I)^{-1}\by \\
    &= K_* (K + \lambda I)^{-1}\by \text{,}
\end{align*} where we define \[
    K_* \coloneqq \phi(\bx)^T \Phi \text{.}
\]

This is very similar to Gaussian Process regression, where the prediction is \[
    f(\bx) = K_*\left(K + \sigma^2 I\right)^{-1}\by \text{.}
\]

Kernelised linear regression is then equivalent to Gaussian Process regression when $\lambda = \sigma^2$.



\section{Gaussian Processes}
Consider the task of reconstructing a function $f: \bbR^d \to y$ from finitely many samples. As a complication, each target sample contains is noisy, with the noise having a variance $\sigma^2$.

More formally, we are given a finite set of tuples\[
    \{ (\vec{x_1}, y_1), \dots, (\vec{x_n}, y_n) \}\text,
\] where \[
    y_i \sim \cN\left(f(\vec{x_i}), \sigma^2\right)\text.
\] We wish to be able to predict $f(\vec{x})$, given an arbitrary $\vec{x}$.

A Gaussian process is a distribution over functions. Formally, a Gaussian process is defined as a `collection of random variables, any finite number of which have a joint Gaussian distribution.'\footnote{C. E. Rasmussen \& C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006, ISBN 026218253X.}

Defining \begin{align*}
    m(\vec{x}) &\coloneqq \bbE[f(\vec{x})] \text, \\
    k(\vec{x}, \vec{x}') &\coloneqq \bbE[(f(\vec{x}) - m(\vec{x}))(f(\vec{x'}) - m(\vec{x'}))],
\end{align*} we write \[
    f(\vec{x}) \sim \cG\cP(m(\vec{x}), k(\vec{x}, \vec{x'}))\text.
\]

We write \begin{align*}
    \vec y &\coloneqq (y_1, \dots, y_n)^\intercal \text, \\
    X &\coloneqq (\vec{x_1}, \dots, \vec{x_n})^\intercal \text, \\
    K(X, X) &\coloneqq \begin{bmatrix}
        k(\vec{x_1}, \vec{x_1}) & \cdots & k(\vec{x_1}, \vec{x_n}) \\
        \vdots & \ddots & \vdots \\
        k(\vec{x_n}, \vec{x_1}) & \cdots & k(\vec{x_n}, \vec{x_n})
    \end{bmatrix}\text.
\end{align*}

Then $\cov(\vec{y}) = K(X, X) + \sigma_n^2I$.

Let $\vec{f_*}$ be the test predictions and $X_*$ be the test inputs. Under the prior, we have that the joint distribution of the observed targets and of the function values at the test locations is \[
    \begin{bmatrix}
        \vec{y} \\ \vec{f_*}
    \end{bmatrix} \sim \cN\left(\vec0, \begin{bmatrix}
        K(X, X) + \sigma_n^2I & K(X, X_*) \\
        K(X_*, X) & K(X_*, X_*)
    \end{bmatrix}\right)\text.
\]

It can then be derived that \[
    \vec{f_*} \mid X, \vec{y}, X_* \sim \cN\left(\overline{\vec{f_*}}, \cov(\vec{f_*})\right)\text,
\] where \[
    \overline{\vec{f_*}} = K(X_*, X)[K(X, X) + \sigma_n^2I]^{-1}\vec{y}\text,
\] and \[
    \cov(\vec{f_*}) = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_n^2I]^{-1}K(X, X_*)\text.
\]

The value for $\vec{f_*}$ that has the highest probability is the mean of the Gaussian from which it is sampled. Hence, we can predict that \begin{equation}
    \label{eq:pred}
    \vec{f_*} \approx K(X_*, X)[K(X, X) + \sigma_n^2I]^{-1}\vec{y}\text,
\end{equation} with the variance for each element of the vector being\[
    \var(\vec{f_*}_i) = \left( K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_n^2I]^{-1}K(X, X_*) \right)_{ii}\text.
\]

One can observe from \cref{eq:pred} that the prediction is a linear combination of the observations $\vec{y}$. This is in contrast to linear regression, where the prediction is a linear combination of the inputs.

\subsection{Equivalence to Kernelised Linear Regression}

Observe that \cref{eq:pred} the equation for kernelised linear regression with regularisation. Thus, Gaussian Process regression and kernelised regular regression always return the same expected value, although kernelised linear regression is unable to estimate the variance of that prediction.

\section{Approximating Gaussian Processes}

Gaussian Process regression relies on our calculation of $(K+\sigma_n^2)^{-1}$, which has a high time complexity\footnote{$O(n^3)$ for a naive implementation; $O(n^{2.373})$ for an optimised method based on the Coppersmithâ€“Winograd algorithm.\jakub{reference}}. This obviously does not scale well to large training sets. Instad, we can approximate Gaussian Process Regression with algorithms of a lower time complexity.

\subsection{My Favourite Approximation}

\jakub{wtf is it called}

We can approximate this matrix product with a complexity that is linear in $n$.

Let $\Phi \in \bbR^{n \times f}$ for $f \ll n$ be our kernel approximation for the training set. Then $K \approx \Phi \Phi^T$. Let $\phi \in \bbR^{f}$ be the kernel approximation for the value we wish to predict for. Then $K_* \approx \Phi \phi$. Hence,\begin{align*}
    K_*^T(K+\sigma_n^2)^{-1}K_* &\approx \phi^T\Phi^T  \left(\Phi \Phi^T + \sigma_n^2I\right)^{-1} \Phi\phi \\
    &= \phi^T\Phi^T  \left( \sigma_n^{-2}I - \sigma_n^{-2}\Phi(\sigma_n^{2} + \Phi^T\Phi)^{-1}\Phi^T\right) \Phi\phi \\
    &=  \sigma_n^{-2}\phi^T\Phi^T\Phi\phi - \sigma_n^{-2}\phi^T\Phi^T \Phi(\sigma_n^{2} + \Phi^T\Phi)^{-1}\phi\Phi^T\Phi
\end{align*}

Then \[
    \hat f(\bx) = \sigma_n^{-2} \phi^T\Phi^T\by - \sigma_n^{-2} \phi^T\Phi^T\Phi(\sigma_n^{2} + \Phi^T\Phi)^{-1}\Phi^T\by
\] and \[
    \var \hat f(\bx) = \phi^T\phi - \sigma_n^{-2}\Phi^T \phi^T\phi\Phi - \sigma_n^{-2}\Phi^T \phi^T\Phi(\sigma_n^{2} + \Phi^T\Phi)^{-1}\Phi^T\phi\Phi\text.
\]

Numerical instabilities present difficulties for small $\sigma_n^2$ as the matrix $(\Phi\Phi^T + \sigma_n^2I)$ becomes closer to being singular.

\subsection{Linear Regression and KDE}

The mean of Gaussian Process Regression can also be approximated by linear regression. Letting $\Phi$ be the kernel approximation on the training set, we find that the regression weights are\[
    \bw = \left(\Phi^\top\Phi + \sigma_n^2 I\right) \Phi^\top\by\text,
\] where $\sigma_n^2$ becomes the regularisation constant. This can be computed in $O(nf)$ time by solving the linear equation instead of directly performing the matrix inversion.

It remains to find an approximation to the variance predicted by Gaussian Process regression. Kernel density estimation can be used to approximate this. Let $D$ be the density estimator. Then we expect $\left(D(\bx)\right)^1$ and $\var f(\bx)$ to have a monotonic relationship.

The actual relationship could probably be found by regression on the estimated densities and the estimated variances of an artificially generated dataset.

This method is less prone to numerical instabilities than my favourite approximation. However it was not used in this work, as a more direct approximation was available.

\section{Finding Hyperparameters}

  \subsection{Grid Search}

  Grid Search is one technique that permits us to find hyperparameters. We must first define a finite set of candidate hyperparameter configurations $H$.

  The algorithm is as follows:
  \begin{minted}[
    gobble=4,
    linenos,
    fontsize=\small,
    frame=lines,
    python3=true
  ]{python}
    def cross_validate(H, train_X, train_y, validation_X, validation_y)
        best_parameters = None
        best_score = float('-inf')

        for h in H:
            regressor.set_params(**h)
            regressor.train(train_X, train_y)
            score = regressor.score(validation_X, validation_y)

            if score > best_score:
                best_parameters = h

        return best_parameters
  \end{minted}

  The technique is called grid search since $H$ is often a cartesian product of the options for each hyperparameter.

  \subsection{Bayesian Optimisation}
  Our Gaussian Process Regression has six hyperparameters: a length scale for each of the five features and a regularisation constant. The high dimensionality of the hyperparameter space makes it difficult to find the optimal parameters using grid search. Bayesian optimisation may be used instead to speed up this process.

\jakub{derive bayesian optimisation}

\subsection{Automatic Relevance Determination}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{passive_delta.pdf}
    \caption{Passive learning curve: Mean $\delta$ error plotted against the number of training points for both SDSS and 2dFLenS. The training points are chosen at random. The shaded interval represents the two-sigma confidence interval.}
    \label{fig:passive_delta}
  \end{figure}

  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{passive_r2.pdf}
    caption{Passive learning curve: $R^2$ error plotted against the number of training points for both SDSS and 2dFLenS. The training points are chosen at random. The shaded interval represents the two-sigma confidence interval.}
    \label{fig:passive_r2}
  \end{figure}

Automatic Relevance Determination is another technique that may be used to find the hyperparameters for our Gaussian Process Regression.

Consider the problem of linear regression. We have a five-dimensional feature space $\cX$. The features have \jakub{noise vairances?} variances $\lambda_1, \dots, \lambda_5$, respectively. We place a prior on our regression weights.

Our prior variance for each weight is proportional to the variance of the weight. We also assume that the weights are uncorrelated. Hence, our sum-of-squares loss function becomes \jakub{how?}\[
    \frac12\norm{X\bw - \by}^2 + \alpha \bw^T \Lambda \bw \text.
\]

\jakub{now talk about that weird convergence bullshit for the regularisation constant}

\jakub{we don't care about pruning right? because we're not pruning...}

\chapter{Active Learning}

\jakub{define active learning}

\jakub{motivate}

\section{Measuring performance}

\jakub{write}

\section{Recommenders}

In active learning, a recommender is a function that scores the utility of each additional point to the regression problem. It can be thought of in two equivalent ways:\begin{itemize}
    \item It assigns a utility score to each candidate point
    \item It returns the top $n$ points from the candidate set
\end{itemize}

\subsection{Uncertainty}
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{uncertainty_r2.pdf}
    \caption{Learning curve: $R^2$ error plotted against the number of training points for both SDSS and 2dFLenS. The training points are chosen either at random (Passive) or using uncertainty recommendation with a batch size of $100$. The shaded interval represents the two-sigma confidence interval.}
    \label{fig:uncertainty_r2}
  \end{figure}

Let $\var \hat f$ be the variance returned by Gaussian PRocess Regression. The uncertainty recommender is defined as \[
    \argmax_{c \in \cC} \var\hat f(c) \text.
\]

Intuitively, it returns the points that we are the most uncertain about. If we are uncertain about a point, then we have less information about that particular area of the feature space. Hence the points that we are uncertain about would be good additions to the training set.

This intuition fails at the start of our training. This is because we end up focusing on points that we are very unsure about, which tend to be on the long tail of the distribution. Since, in our case, the distribution is five-dimensional, this is a very poor strategy, as we spread out our efforts and neglect the core of the distribution.

\jakub{learning curve}

\subsection{Density}

Let $\cD : \cX \to \bbR^+$ be a density function for the dataset. This can be derived from the training set by fitting a gaussian mixture model or by Kernel Density Estimation. The recommended points are \[
    \argmax_{c \in \cC} \cD(c) \text.
\]

\jakub{learning curve on pure density}

As seen above, this recommendation strategy yields good results at the start of the learning curve, when we have trained on very few datapoints. However, late on the curve flattens out and no longer produces very good results, being overtaken by uncertainty recommendation.

\subsection{Boltzmann sampling}

Boltzmann sampling permits us to obtain a variety of training points from one recommender.

\jakub{derive}

\jakub{this really needs more formal vocabulary}

It prevents `clumping' of points, and hence redundant information. It is also useful when recommending in batches. Since scores are continuous, two recommended data points are correlated, producing redundant information; boltzmann sampling helps reduce this.

\jakub{learning curves of boltzmann on uncertainty and density with different temperatures}

\subsection{Mixing recommenders}

From the plots above, we see that different recommenders excel at different stages of the training. Density recommendation is best when we are starting off, and uncertainty recommendation helps us tackle the long tail of the distribution in the later stages. The need to combine the benefits of both strategies becomes clear.

There are two ways of combining recommenders, depending on their definition.

If we think of a recommender as a function that assigns a score to a candidate, and we are given two recommenders $A : \cC \to \bbR$ and $B : \cC \to \bbR$, then we can derive a third recommender $C : \cC \to \bbR$ as \[
    C(c) \coloneqq kA(c) + lB(c)
\] for come $k, l \in \bbR$.

This is not always the most helpful strategy. The distribution of scores returned by $A$ and $B$ could be totally different, in which case it doesn't make sense to scale each by a constant and sum them. They need to be on the same order and on the same scale.

Instead, another way of deriving a mixed recommender is to think about probabilities. When selecting $n$ elements from $C$, we want to select elements from $A$ with probability $p$ and elements from $B$ with probability $1-p$. Hence, we can combine $pn$ elements from $A$ and $(1-p)n$ elements from $B$ to form the $n$ elements from $C$.

A constant $p$ (or equivalently constant $k$ and $l$) with combine both the advantages and disadvantages of each strategy, yielding a mediocre recommender. We know, however, that uncertainty recommendation and density recommendation excel at different stages of training. It makes sense to prioritise density recommendation at the start, and uncertainty recommendation towards the end.

This is reminiscent of the problem of exploration vs exploitation. Uncertainty scores are mostly noise until we have enough data in our training set.

\jakub{figure out optimal function for p/kl}

\jakub{helpful plot}

\bibliography{references}{}
\bibliographystyle{plain}

\end{document}
