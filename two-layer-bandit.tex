%% Submissions for peer-review must enable line-numbering
%% using the lineno option in the \documentclass command.
%%
%% Preprints and camera-ready submissions do not need
%% line numbers, and should have this option removed.
%%
%% Please note that the line numbering option requires
%% version 1.1 or newer of the wlpeerj.cls file.

\documentclass[fleqn,10pt,lineno]{wlpeerj} % for journal submissions
% \documentclass[fleqn,10pt]{wlpeerj} % for preprint submissions


\newcommand{\cheng}[1]{\textcolor{green}{\textbf{Cheng: }{\footnotesize #1}}}
\newcommand{\alasdair}[1]{\textcolor{blue}{\textbf{Alasdair: }{\footnotesize #1}}}

\usepackage{bm} % bold math symbols

% some convenient symbols
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\arsinh}{arsinh}
\DeclareMathOperator{\tr}{tr}
\newcommand{\A}{\mathpzc{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\Unlabelled}{\mathcal{U}}
\newcommand{\Labelled}{\mathcal{L}}
\newcommand{\R}{\mathcal{R}}
\newcommand*{\argmin}{\operatornamewithlimits{argmin}\limits}
\newcommand*{\argmax}{\operatornamewithlimits{argmax}\limits}


\title{Combining Active Learning Suggestions}

\author[1]{Alasdair Tran}
\author[2]{Cheng Soon Ong}
\affil[1]{}
\affil[2]{Machine Learning Research Group, Data61, CSIRO, Australia}

\keywords{machine learning, astronomy, active learning, bandit, rank aggregation}

\begin{abstract}
Recent advances in sensors and scientific instruments have led to an increasing use of machine learning techniques for managing the data deluge. Supervised learning has become a widely used paradigm in many big data applications. However,  labeled examples are required during the training phase of supervised machine learning algorithms, and the labeling has become a significant bottleneck. This paper explores the use of machine learning algorithms for identifying informative examples for labeling, the so-called active learning setting. We empirically compare several active learning heuristics on benchmark datasets, and focus on its application to photometric classification of the Sloan Digital Sky Survey. By considering each active learning heuristic as an expert recommendation of which example to label, we propose to combine them using bandit and rank aggregation algorithms. Our results show that combining active learning suggestions improves over each individual heuristic (including passive learning), and provides a promising practical approach.
\end{abstract}

\begin{document}

\flushbottom
\maketitle
\thispagestyle{empty}

\section*{Introduction}


Previous surveys~\cite{baram04onlcal,hsu15actll}. We are more comprehensive, and consider other types of combination.

There are three ideas which are often used for eliciting human
responses using machine learning predictors. At a high level they are
similar is spirit, but they have different foundations which lead to
different formulations. The ideas are active learning, bandits and
experimental design. Related to this but with literature from a different field is social choice theory, which looks at how individual preferences are aggregated.



\section*{Binary and Multiclass classification}

\begin{itemize}
  \item binary classification
  \item multiclass classification
\end{itemize}

Base learner: Logistic regression
\begin{itemize}
  \item Warm start?
  \item polynomial kernel of degree 2
\end{itemize}

Performance:
\begin{itemize}
  \item Accuracy
  \item Posterior mean balanced accuracy
  \item F1 score
\end{itemize}

Random sampling, figure out asymptote of performance.

\section*{Active Learning, Bandits, Choice and Design of Experiments}

\subsection*{Active Learning}

Active learning considers the setting where the agent interacts with
its environment to procure a training set, rather than passively
receiving i.i.d. samples from some underlying distribution.

It is often assumed that the environment is infinite (e.g. $R^d$) and
the agent has to choose a location, $x$, to query. The oracle then returns
the label $y$. It is often assumed that there is no noise in the label,
and hence there is no benefit of querying the same point $x$ again. In
many practical applications, the environment is considered to be
finite (but large). This is called the pool-based active learning.

The active learning algorithm is often compared to the passive
learning algorithm.

Reward: improvement in performance.

\begin{table}[h]
	\caption {Summary of active learning heuristics used in our experiments} \label{tab:heuristics}
	\centering
	\begin{tabular}{lll}
		\toprule
		{Name}  & Notation &  Objective  \\
		\midrule
		Entropy & $r_S(\bm{x}; h)$
			& $\argmax_{x \in \Ecal} \left\{-\sum_{y \in \Y} p(y | \bm{x}; h)
            \log \big[ p(y | \bm{x}; h) \big] \right\}$
			\\[2ex]
		Margin & $r_M(\bm{x}; h)$
			& $\argmin_{x \in \Ecal} \left\{ \max_{y \in \Y} p(y | \bm{x}; h) -
            \max_{z \in \Y \setminus \{y\}} p(z | \bm{x}; h)  \right\}$
			\\[2ex]
		QBB Margin & $r_{QM}(\bm{x}; h)$
			& $\argmin_{x \in \Ecal} \left\{ \max_{y \in \Y} p(y | \bm{x}; \B) -
            \max_{z \in \Y \setminus \{y\}} p(z | \bm{x}; \B)  \right\}$
			\\[2ex]
		QBB KL & $r_{QK}(\bm{x}; h)$
			& $\argmax_{x \in \Ecal} \left\{ \dfrac{1}{B}
               \sum_{b=1}^B D_{\mathrm{KL}}(p_b\|p_\B) \right\}$
			\\
		\bottomrule
	\end{tabular}
\end{table}



\begin{itemize}
  \item (slow) Reduction of expected variance
  \item (slow) Reduction of expected entropy
  \item (maybe) information density http://burrsettles.com/pub/settles.emnlp08.pdf
\end{itemize}

\subsection*{Bandits}

A bandit problem is a sequential allocation problem defined by a set
of actions. The agent chooses an action at each time step, and the
environment returns a reward. The aim of the agent is to maximise reward.

In basic settings, the set of actions is considered to be
finite. There are three fundamental formalisations of the bandit
problem, depending on the assumed nature of the reward process:
stochastic, adversarial and Markovian. In all three settings the
reward is uncertain, and hence the agent may have to play a particular
action repeatedly.

The agent is compared to a static agent which has played the best
action. This difference in reward is called regret.

\begin{itemize}
  \item survey http://research.microsoft.com/en-us/um/people/sebubeck/SurveyBCB12.pdf
  \item kl-UCB http://arxiv.org/pdf/1210.1136.pdf
  \item OC-UCB http://arxiv.org/pdf/1507.07880v2.pdf
  \item EXP3++ http://jmlr.org/proceedings/papers/v32/seldinb14.pdf
  \item Thompson sampling
\end{itemize}

\subsection*{Choice: Rank aggregation}

http://plato.stanford.edu/entries/social-choice/

\begin{itemize}
  \item Pairwise majority rule
  \item Borda count
  \item geometric mean http://arxiv.org/abs/1410.4391
\end{itemize}


\subsection*{Experimental Design}

In contrast to active learning, experimental design considers the problem of regression, i.e. where the label $y\in R$ is a real number.

The problem to be solved in experimental design is to choose a set of
trials (say of size N) to gather enough information about the object
of interest. The goal is to maximise the information obtained about
the parameters of the model (of the object).

It is often assumed that the observations at the N trials are
independent. When N is finite this is called exact design, otherwise
it is called approximate or continuous design. The environment is
assumed to be infinite (e.g. $R^d$) and the observations are scalar real variables.




\section*{Empirical comparison}

\subsection*{Description of datasets}

\subsubsection*{UCI Data}


\cheng{This should be a table in the appendix.}
\begin{description}
  \item[binary classification]\
  \begin{itemize}
      \item [ionosphere](https://archive.ics.uci.edu/ml/datasets/Ionosphere): Radar data.
      \item [pima](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes): Pima Indians Diabetes.
      \item [sonar](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)): We want to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.
      \item [wpbc](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)): Prognostic Wisconsin breast cancer.
  \end{itemize}
  \item[multiclass]\
  \begin{itemize}
    \item [iris](https://archive.ics.uci.edu/ml/datasets/Iris): Well-known dataset from Fisher, three classes.
    \item [glass](https://archive.ics.uci.edu/ml/datasets/Glass+Identification): Glass identification, seven classes.
    \item [vehicle](https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes)): Classifying a given sihouette as one of four types of vehicle.
    \item [wine](https://archive.ics.uci.edu/ml/datasets/Wine): Using chemical analysis to determine the origin of wines.
  \end{itemize}
\end{description}

\subsubsection*{SDSS}

\begin{itemize}
  \item binary classification: stars vs galaxies
  \item multiclass: stars vs galaxies vs quasars
\end{itemize}

\section*{Bits to tidy up, Appendix?}
\begin{itemize}
  \item Posterior balanced Accuracy derivation
  \item Reddening correction
  \item Feature selection, best kernel is poly degree 2
  \item Choose value of C
\end{itemize}


\section*{Acknowledgments}

So long and thanks for all the fish.

\bibliography{active}

\end{document}
