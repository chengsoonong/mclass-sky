%% Submissions for peer-review must enable line-numbering
%% using the lineno option in the \documentclass command.
%%
%% Preprints and camera-ready submissions do not need
%% line numbers, and should have this option removed.
%%
%% Please note that the line numbering option requires
%% version 1.1 or newer of the wlpeerj.cls file.

\documentclass[fleqn,10pt,lineno]{wlpeerj} % for journal submissions
% \documentclass[fleqn,10pt]{wlpeerj} % for preprint submissions

\newcommand{\cheng}[1]{\textcolor{green}{\textbf{Cheng: }{\footnotesize #1}}}
\newcommand{\alasdair}[1]{\textcolor{blue}{\textbf{Alasdair: }{\footnotesize #1}}}

\usepackage{bm} % bold math symbols
\usepackage{hyperref} % create hyperlinks
\usepackage[labelfont=bf]{caption} % figure captions
\usepackage{algorithm} % display algorithms blocks
\usepackage{algpseudocode} % display algorithms in pseudo-code
\usepackage{IEEEtrantools} % more flexible formatting of equations
\usepackage{mathtools} % use DeclarePairedDelimiterXPP 

\hypersetup{
	colorlinks   = true, % Colours links instead of ugly boxes
	pdfauthor    = {Alasdair Tran and Cheng Soon Ong},
	pdftitle	 = {Combining Active Learning Suggestions},
}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n} % reset caligraphy font

% some convenient symbols
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\tr}{tr}
\newcommand{\A}{\mathpzc{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\Unlabelled}{\mathcal{U}}
\newcommand{\Labelled}{\mathcal{L}}
\newcommand{\R}{\mathcal{R}}
\newcommand*{\argmin}{\operatornamewithlimits{argmin}\limits}
\newcommand*{\argmax}{\operatornamewithlimits{argmax}\limits}
\newcommand{\MPBA}{\text{MPBA}}
\newcommand{\passive}{\text{passive}}

% Expected value
\providecommand\given{}
\DeclarePairedDelimiterXPP\E[1]{\mathbb{E}}{[}{]}{}{
	\renewcommand\given{  \nonscript\:
		\delimsize\vert
		\nonscript\:
		\mathopen{}
		\allowbreak}
	#1
}
% Probability
\DeclarePairedDelimiterXPP\Prob[1]{\mathbb{P}}{(}{)}{}{
	\renewcommand\given{  \nonscript\:
		\delimsize\vert
		\nonscript\:
		\mathopen{}
		\allowbreak}
	#1
}

\title{Combining Active Learning Suggestions}

\author[1]{Alasdair Tran}
\author[2]{Cheng Soon Ong}
\author[3]{Christian Wolf}
\affil[1]{}
\affil[2]{Machine Learning Research Group, Data61, CSIRO, Australia}
\affil[3]{Research School of Astronomy and Astrophysics, Australian National University}

\keywords{machine learning, astronomy, active learning, bandit, rank aggregation}

\begin{abstract}
Recent advances in sensors and scientific instruments have led to an increasing
use of machine learning techniques for managing the data deluge. Supervised
learning has become a widely used paradigm in many big data applications.
However,  labeled examples are required during the training phase of supervised
machine learning algorithms, and the labeling has become a  significant
bottleneck. This paper explores the use of machine learning algorithms for
identifying informative examples for labeling, the so-called active learning
setting. We  empirically compare several active learning heuristics on
benchmark UCI datasets, and focus on its  application to photometric classification
of the Sloan Digital Sky Survey. By considering each  active learning heuristic
as an expert recommendation of which example to label, we propose to  combine
them using bandit and rank aggregation algorithms. Our results show that
combining  active learning suggestions improves over each individual heuristic
(including passive  learning), and provides a promising practical approach.

\end{abstract}

\begin{document}

\flushbottom
\maketitle
\thispagestyle{empty}

\section*{Introduction}

There are three ideas which are often used to elicit human responses - active
learning, bandits, and experimental designs. Although similar in spirit, these
ideas have different foundations which lead to different formulations. Related
to this but with literature from a different field is social choice theory,
which looks at how individual preferences are aggregated.

Active learning considers the setting where the agent interacts with its
environment to procure a training set. This contrasts with passive learning,
where the agent passively receives i.i.d. samples from some underlying
distribution. The environment is usually assumed to be infinite and the agent
has to choose a location to query an oracle for a label. It is often assumed
that there is no noise in the label, and hence there is no benefit of querying
the same point again. In many practical applications, the environment is
considered to be finite (but large). This is called the pool-based active
learning.

A bandit problem is a sequential allocation problem defined by a set of
actions. The agent chooses an action at each time step, and the environment
returns a reward. The aim of the agent is to maximise reward. In basic
settings, the set of actions is considered to be finite. There are three
fundamental formalisations of the bandit problem, depending on the assumed
nature of the reward process: stochastic, adversarial and Markovian. In all
three settings the reward is uncertain, and hence the agent may have to play a
particular action repeatedly. The agent is compared to a static agent which has
played the best action. This difference in reward is called regret.

In contrast to active learning, experimental design considers the problem of
regression, i.e. where the label $y\in R$ is a real number. The problem to be
solved in experimental design is to choose a set of trials (say of size N) to
gather enough information about the object of interest. The goal is to maximise
the information obtained about the parameters of the model (of the object). It
is often assumed that the observations at the N trials are independent. When N
is finite this is called exact design, otherwise it is called approximate or
continuous design. The environment is assumed to be infinite (e.g. $R^d$) and
the observations are scalar real variables.

Previous surveys~\cite{baram04, hsu15}.
We are more comprehensive, and consider other types of combination.

\section*{Problem Formulation}

\subsection*{Active Learning Suggestions}


Suppose we have a hypothesis $h$ that maps the input space $\X \subset
\mathbb{R}^d$ to the label space $\Y$. We require that a small number of
objects have already been labelled--these become the labelled set $\Labelled
\subset \X \times \Y$. The rest form the unabelled set $\Unlabelled \subset
\X$. Now consider the problem of choosing the next example in $\Unlabelled$ for
querying, using a rule $r(\bm{x}; h)$ that gives each unlabelled example a
score based only on their feature vector $\bm{x}$ and the current hypothesis
$h$. Coming up with a good rule is itself a hard problem, but there have been
many attempts to derive good heuristics. Five common ones are listed in Table
\ref{tab:heuristics}. They roughly fall into two categories: uncertainty
sampling and version space reduction.

\cite{lewis94} introduce the idea of uncertainty sampling, where we select the
example whose class membership the classifier is least certain about. These
tend to be points that are near the decision boundary of the classifier.
Perhaps the simplest way to quantify uncertainty is \cite{culotta05}'s least
confidence heuristic, where we pick the candidate whose most likely label the
classifier is most uncertain about. A second option is to calculate the entropy
\citep{shannon48}, which measures the amount of information needed to encode a
distribution. Intuitively, the closer class probabilities of an object are to
random guessing, the higher its entropy will be. This gives us the heuristic of
picking the candidate with the highest entropy. Finally we can pick the
candidate with the smallest margin, which is defined to be the difference
between the two highest class probabilities \citep{scheffer01} Since the sum of
all probabilities must be 1, the smaller the margin is, the more uncertain we
are about the object's class membership.


Instead of focussing on the uncertainty of individual predictions, we could
instead try to constrain the size of the version space, thus allowing the
search for the optimal hypothesis to be more precise. The version space is
defined as the set of all possible hypotheses that are consistent with the
current training set. To quantify the size the space, we can train a committee
of classifiers, $\B = \{h_1, h_2, ..., h_B\}$, and measure the disagreement
among the members about an object's class membership. Each committee member
needs to have a hypothesis that is as different from the others as possible but
that is still in the version space \citep{melville04}. In order to have this
diversity, we give each member only a subset of the training examples. Since
there might not be enough training data, we need to use bootstrapping and
select samples with replacement. Hence this method is often called Query by
Bagging (QBB).

One way to measure the level of disagreement is to calculate the margin using
the class probabilities estimated by the committee \citep{melville04}. This
looks similar to one of the uncertainty sampling heuristics, except now we
first average out the probabilities of the members before minimising the
margin. \cite{mccallum98} offer an alternative disagreement measure which
involves picking the candidate with the largest expected Kullback--Leibler (KL)
divergence from the average.


\begin{table}[h]
	\caption {Summary of active learning heuristics used in our experiments} \label{tab:heuristics}
	\centering
	\begin{tabular}{lll}
		\toprule
		{Heuristic}  &  Objective  \\
		\midrule
        Least Confidence &
			$\argmax_{x \in \Ecal} \left\{ \max_{y \in \Y} p(y | \bm{x}; \B) \right\}$
			\\
		Highest Entropy &
			$\argmax_{x \in \Ecal} \left\{-\sum_{y \in \Y} p(y | \bm{x}; h)
            \log \big[ p(y | \bm{x}; h) \big] \right\}$
			\\[2ex]
		Smallest Margin &
			$\argmin_{x \in \Ecal} \left\{ \max_{y \in \Y} p(y | \bm{x}; h) -
            \max_{z \in \Y \setminus \{y\}} p(z | \bm{x}; h)  \right\}$
			\\[2ex]
		Smallest QBB Margin &
			$\argmin_{x \in \Ecal} \left\{ \max_{y \in \Y} p(y | \bm{x}; \B) -
            \max_{z \in \Y \setminus \{y\}} p(z | \bm{x}; \B)  \right\}$
			\\[2ex]
		Largest QBB KL &
			$\argmax_{x \in \Ecal} \left\{ \dfrac{1}{B}
               \sum_{b=1}^B D_{\mathrm{KL}}(p_b\|p_\B) \right\}$
			\\
		\bottomrule
	\end{tabular}
\end{table}

\subsection*{Combining the Suggestions with Bandits}

Out of the five heuristics discussed, how do we know which one is the optimal,
anyway? There have been some attempts in the literature to do a theoretical
analysis of them. However proofs are scarce, and when there is one available,
they normally only work under simplifying assumptions. For example,
\cite{freund97} show that the query by committee algorithm (a slight variant of
our QBB heuristics) guarantees an exponential decrease in the prediction error
with the training size, but only under certain restrictions such as there is no
noise. Thus whether any of these heuristics is guaranteed to beat random
sampling is still an open question. We shall not worry too much about the
theoretical analysis in this thesis. Instead we shall focus on an empirical
analysis in the astronomical domain.

To help us automatically choose the optimal heuristic, we now turn our
attention to the multi-armed bandit problem in probability theory. The
colourful name originates from the situation where a gambler stands in front of
a slot machine with $s$ levers. When pulled, each lever gives out a random
reward according to some unknown distribution. The goal of the game is to come
up with a strategy that can maximise the gambler's lifetime rewards while
minimising the number of pulls.

Suppose we have a set of $s$ heuristics $ \R = \{r_1, ..., r_s \}$. Each
heuristic has a different ability to identify the candidate whose labelling
information is most valuable. An appropriate reward is then the incremental
increase in the accuracy rate after the candidate is added to the training set.
We assume that the heuristic rewards are independent of each other. This is
reasonable since the theories with which we use to derive the heuristics are
mostly unrelated.

One problem in multi-arm bandits is the trade-off between exploring random
heuristics and exploiting the best heuristic so far. There are many instances
in which we find our previously held beliefs to be completely wrong. Thus by
always exploiting, we could miss out on the optimal heuristic. On the other
hand, if we explore too much, it might take a long time to reach the desired
accuracy and the strategy ends up being no different from random sampling.

We shall compare four algorithms that address this exploration-vs-exploitation
problem. The oldest of these is Thompson sampling, \citep{thompson33},
that solves the trade-off from a Bayesian
perspective. We also
consider Optimally Confident UCB \citep{lattimore15} and KL-UCB \citep{cappe13},
which were inspired by \citep{auer02}'s UCB algorithm. Finally we consider
EXP3++ \citep{seldin14}. 



\subsection*{Combining the Suggestions with Social Choice Theory}

A drawback of the bandit methods is that at each iteration, we could only use
information from one particular suggestion. Since most suggestions have
requires little computation, we could in fact run all of them and somehow
combine the rankings. This leads us to Social Choice Theory.

Pairwise majority rule.

Borda count.

Geometric mean \cite{bedo14}


\section*{Empirical comparison}

\subsection*{Description of datasets}

\begin{table}[h]
	\caption {Overview of datasets} \label{tab:datasets}
	\centering
	\begin{tabular}{lrrrr}
		\toprule
		{Dataset}  & Size &  Classes & Majority & Max MPBA \\
		\midrule
        \href{https://archive.ics.uci.edu/ml/datasets/Glass+Identification}{glass}
        	& $214$ & $6$ & $33\%$ & $65\%$ \\
		\href{https://archive.ics.uci.edu/ml/datasets/Ionosphere}{ionosphere}
        	& $351$ & $2$ & $64\%$ & $89\%$ \\
        \href{https://archive.ics.uci.edu/ml/datasets/Iris}{iris}
        	& $150$ & $3$ & $33\%$ & $90\%$ \\
        \href{https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope}{magic}
        	& $19~020$ & $2$ & $65\%$ & $84\%$ \\
        \href{https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification}{miniboone}
        	& $129~596$ & $2$ & $72\%$ & $88\%$ \\
        \href{https://archive.ics.uci.edu/ml/datasets/Page+Blocks+Classification}{pageblock}
        	& $5~473$ & $5$ & $90\%$ & $79\%$ \\
		\href{https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes}{pima}
        	& $733$ & $2$ & $66\%$ & $71\%$ \\
        \href{http://dx.doi.org/10.5281/zenodo.58500}{sdss}
        	& $2~801~002$ & $3$ & $61\%$ & $90\%$ \\
		\href{https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)}{sonar}
        	& $208$ & $2$ & $53\%$ & $78\%$ \\
        \href{https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes)}{vehicle}
        	& $846$ & $4$ & $26\%$ & $81\%$ \\
        \href{https://archive.ics.uci.edu/ml/datasets/Wine}{wine}
        	& $178$ & $3$ & $40\%$ & $94\%$ \\
		\href{https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)}{wpbc}
        	& $194$ & $2$ & $76\%$ & $58\%$ \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection*{Experimental Setup}
We use a 10-fold stratified shuffled split cross validation. The unlabelled
pool size is 70\% of data up to a maximum of 10,000 examples. The test pool
size is the remaining examples up to a maximum of 20,000. We use logistic
regression with a Gaussian kernel approximation and an L2 loss.

We shall use the mean posterior balanced accuracy (MPBA) as the reward. Refer
to the appendix for the MPBA derivation.

Given that there are 12 datasets, each with 16 learning curves, we need a
measure that can summarise in one number how well a particular suggestion or
policy does. Building on \cite{baram04}'s deficiency measure, we define the
strength of an active learner or policy, relative to passive learning, as

\begin{IEEEeqnarray*}{lCl}
    \text{Strength}(h) &=&
    	1 - \dfrac{\sum_{t=1}^{n}\big(\MPBA(\max) - \MPBA(h, t)\big)}
    	{\sum_{t=1}^{n}\big(\MPBA(\max) - \MPBA(\passive, t)\big)}
\end{IEEEeqnarray*}
Intuitively, this relates to the area between a target learning curve and the
benchmark passive curve.

\section*{Discussion}

In practice, we do not have a representative test set that can be used to
compute the reward. Thus aggregation methods like Borda are are suitable than
bandit algorithms such as Thompson sampling. In the absence of such a test set,
\cite{hsu15} addressed this problem by using importance weighting to remove the
bias in the accuracy on the training set. For this to work, however, we need to
let every training example and active learning suggestion have a non-zero
probability of being selected in every iteration.



\begin{figure}[tbp]
	\centering
	\includegraphics[width=\textwidth]{figures/strengths}
	\caption[Policy strength]{Strength of policies and heuristics relative to passive learning}
	\label{fig:strengths}
\end{figure}


\begin{figure}[tbp]
	\centering
	\includegraphics[width=\textwidth]{figures/learning_curves}
	\caption[Selected learning curves]{Selected learning curves. As it would get too cluttered to plot 16 learning curves, we only show the MPBA curves for the least confidence heuristic, the EXP3++ bandit policy, and the Borda aggregation policy.}
	\label{fig:learning_curves}
\end{figure}


\section*{Acknowledgments}

SDSS

\bibliography{active}

\pagebreak

\section*{Appendix A: Algorithms}

\algblock[Name]{Start}{End}
\algblockdefx[Forall]{Foreach}{Endforeach}%
			[1]{\textbf{for each} #1 \textbf{do}}%
			{\textbf{end for}}

\begin{algorithm}[tbp]
	\caption{Thompson sampling} \index{bandit}
	\label{alg:bandit}
	\begin{algorithmic}[1]
		\Procedure {ThompsonSampling}{$\Unlabelled$, $\Labelled_T$, $h$, $n$, $E$,
			                      $\R$, $\bm{\mu}$, $\bm{\sigma}^2$, $\bm{\tau}^2$}
    		\While {$|\Labelled_T| < n$}
        		\Foreach  {$i \in \{1, 2, ..., |\R|\}$}
        			\State $\nu_i' \leftarrow$ draw a sample from $\Normal(\mu_i, \sigma^2_i)$
        		\Endforeach
        		\State $r_* \leftarrow \argmax_{i} \nu_i'$
        		\State $\Ecal$ $\leftarrow$ random sample of size $E$ from $\Unlabelled$
        		\State $\bm{x}_* \leftarrow \argmax_{\bm{x} \in \Ecal} r_*(\bm{x})$
        		\State $y_* \leftarrow$ ask the expert to label $\bm{x}_*$
        		\State $\Labelled_T \leftarrow \Labelled_T  \cup (\bm{x}_*, y_*)$
        		\State $\Unlabelled \leftarrow \Unlabelled \setminus \bm{x}_*$
        		\State $h(\bm{x}) \leftarrow$ retrain the classifier
        		\State $\delta$ $\leftarrow$ incremental increase in the accuracy
        		\State $\mu_* \leftarrow \dfrac{\mu_* \tau^2_* + \delta \sigma^2_*}{\sigma^2_* + \tau^2_*}$
                \State $\sigma_*^2 \leftarrow \dfrac{\sigma^2_* \tau^2_*}{\sigma^2_* + \tau^2_*}$
    		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\section*{Appendix B: Posterior Balanced Accuracy}


Most real-world datasets are unbalanced. In the SDSS dataset, for example,
there are 4.5 times as many galaxies as quasars. The problem of class imbalance
is even more severe in the pageblock dataset, where one class makes up 90\% of
the data and the remaining four classes only make up the remaining 10\%. An
easy fix is to undersample the dominant class when creating training and test
sets. This, of course, means that the size of these sets are limited by the
size of the minority class.

When we do not want to alter the underlying class distributions or when larger
training and test sets are desired, we need a performance measure that can
correct for the class imbalance. \cite{brodersen10} show that the posterior
balanced accuracy distribution can overcome the bias in the binary case. We now
extend this idea to the multi-class setting.

Suppose we have $k$ classes. For each class $i$ between $1$ and $k$, there are
$N_i$ objects in the universe. Given a hypothesis, we can predict the label of
every object and compare our prediction to the true label. Let $G_i$ be the
number of objects in class $i$ that are correctly predicted.
Then we define the recall $A_i$ of class $i$ as \index{recall}
	\begin{IEEEeqnarray*}{lCl}
		A_i &=& \frac{G_i}{N_i}
	\end{IEEEeqnarray*}
The problem is that it is not feasible to get the actual values of $G_i$ and
$N_i$ since that would require us to obtain the true label of every object.
Thus we need a method to estimate these quantities when we only have a sample.
Initially we have no information about $G_i$ and $N_i$, so we can assume that
each $A_i$ follows a uniform prior from 0 to 1. This is the same as a Beta
distribution with shape parameters $\alpha = \beta = 1$:
	\begin{IEEEeqnarray*}{lCl}
		A_i &\sim& \Beta(1,1) 
	\end{IEEEeqnarray*}
The PDF of $A_i$ is then
    \begin{IEEEeqnarray}{lCl}
        f_{A_i}(a) &=& \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, 
        a^{\alpha-1}(1-a)^{\beta-1} \label{eqn:prior} \\
        &\propto&   a^{1-1}(1-a)^{1-1}  \notag
    \end{IEEEeqnarray}
where $\Gamma(\alpha)$ is the gamma function.

After we have trained the classifier, suppose we have a test set containing
$n_i$ objects in class $i$. Running the classifier on this test set is the same
as conducting $k$ binomial experiments, where, in the $i$th experiment, the
sample size is $n_i$ and the probability of success is simply $A_i$. Let $g_i$
be the number of correctly labelled objects belonging to class $i$ in the test
set. Then, conditional on the accuracy rate, $g_i$ follows a binomial
distribution:
	\begin{IEEEeqnarray*}{lCl}
		(g_i \mid A_i) &\sim& \Bin(n_i, A_i)
	\end{IEEEeqnarray*}
The probability mass function of $(g_i \mid A_i = a)$ is thus
    \begin{IEEEeqnarray}{lCl}
        p_{g_i \mid A_i}(g_i) &=& \binom{n_i}{g_i} a^{g_i} (1 - a)^{n_i - g_i} \label{eqn:likelihood} \\
                              &\propto& a^{g_i} (1 - a)^{n_i - g_i} \notag
    \end{IEEEeqnarray}
In the Bayesian \index{Bayesian} setting, \eqref{eqn:prior} is the prior and
\eqref{eqn:likelihood} is the likelihood. To get the posterior PDF, we simply
multiply the prior with the likelihood:
	\begin{IEEEeqnarray*}{lCl}
		f_{A_i \mid \bm{g}}(a)
		&\propto& f_{A_i}(a) \times f_{g_i \mid A_i}(g_i) \\
		&\propto& a^{1-1}(1-a)^{1-1} \times a^{g_i} (1 - a)^{n_i - g_i} \\
		&=& a^{1 + g_i - 1}(1-a)^{1 + n_i - g_i - 1}
	\end{IEEEeqnarray*}
Thus, with respect to the binomial likelihood function,
the Beta distribution is conjugate to itself. The posterior recall rate $A_i$
also follows a Beta distribution, now with parameters
	\begin{IEEEeqnarray*}{lCl}
		(A_i \mid g_i) &\sim& \Beta(1 + g_i, 1 + n_i - g_i)
	\end{IEEEeqnarray*}
Our goal is to have a balanced accuracy rate, $A$, that puts an equal weight in
each class. One way to achieve this is to take the average of the individual
recalls:
	\begin{IEEEeqnarray*}{lCl}
		A &=& \frac{1}{k} \sum_{i=1}^k A_i \\
		&=& \frac{1}{k} A_T
	\end{IEEEeqnarray*}
Here we have defined $A_T$ to be the sum of the individual recalls. We call
$(A \mid \bm{g})$ the posterior balanced accuracy (PBA), where $\bm{g}
=(g_1,...,g_k)$. Most of the time, we simply want to calculate its expected
value:
	\begin{IEEEeqnarray*}{lCl}
		\E{A \given \bm{g}} &=& \frac{1}{k} \, \E{A_T \given \bm{g}} \\
		&=& \frac{1}{k} \int a \cdot f_{A_T \mid \bm{g}}(a) \, da
	\end{IEEEeqnarray*}
Let us call this the mean posterior balanced accuracy (MPBA). Note that there
is no closed form solution for the PDF $f_{A_T \mid \bm{g}}(a)$. However
assuming that $A_T$ is a sum of $k$ independent Beta random variables, $f_{A_T
\mid \bm{g}}(a)$ can be approximated by numerically convolving $k$ Beta
distributions. The independence assumption is reasonable here, since there
should be little to no correlation between the individual class accuracy rates.
Knowing that a classifier is really good at recognising stars does not tell us
much about how well that classifier can recognise galaxies.

Having the knowledge of $f_{A \mid \bm{g}}(a)$ will allow us to make violin
plots, construct confidence intervals and do hypothesis tests. To get an
expression for this, let us first rewrite the cumulative distribution function
(CDF) as
	\begin{IEEEeqnarray*}{lCl}
		F_{A\mid \bm{g}}(a) &=& \Prob{A \leq a \mid \bm{g}} \\
		&=& \Prob[\Big]{\frac{1}{k} A_T \leq a \given \bm{g}} \\
		&=& \Prob{A_T \leq ka \given \bm{g}} \\
		&=& F_{A_T \mid \bm{g}}(ka) \IEEEyesnumber \label{eqn:CDF}
	\end{IEEEeqnarray*}
Differentiating \eqref{eqn:CDF} with respect to $a$, we obtain the PDF of $(A \mid \bm{g})$:
	\begin{IEEEeqnarray*}{lCl}
		f_{A \mid \bm{g}}(a) &=& \frac{\partial}{\partial a} F_{A \mid \bm{g}}(ka) \\
		&=& \frac{\partial}{\partial a} (ka) \cdot \frac{\partial}{\partial ka} F_{A_T \mid \bm{g}}(ka) \\
		&=& k \cdot f_{A_T \mid \bm{g}}(ka)
	\end{IEEEeqnarray*}
We shall use the posterior balanced accuracy rate throughout the experiments to
report the overall performance of a classifier on a test set.

\end{document}
